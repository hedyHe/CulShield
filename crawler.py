#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Cultural Atlas æ·±åº¦ç¦å¿Œå†…å®¹çˆ¬è™«ï¼ˆæ•´æ®µæå–ç‰ˆï¼‰
"""
import requests
from bs4 import BeautifulSoup
import re
import json
import csv
import time
from urllib.parse import urljoin
from collections import defaultdict

class CulturalAtlasDeepCrawler:
    def __init__(self):
        self.base_url = "https://culturalatlas.sbs.com.au"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Connection': 'keep-alive',
        })
        self.subcategories = [
            'greetings', 'religion', 'family', 'naming', 'dates-of-significance',
            'etiquette', 'do-s-and-don-ts', 'communication', 'other-considerations', 'business-culture'
        ]
        self.taboo_keywords = [
            r"\bdon't\b", r"\bdon\'t\b", r"\bavoid\b", r"\bnever\b", r"\bshouldn't\b", r"\bshouldn\'t\b", r"\bwon't\b", r"\bwon\'t\b",
            r"\bcan't\b", r"\bcan\'t\b", r"\bmustn't\b", r"\bmustn\'t\b", r"\bforbidden\b", r"\bprohibited\b", r"\bnot allowed\b", r"\bnot acceptable\b",
            r"\billegal\b", r"\bunlawful\b", r"\bdo not\b", r"\bdo NOT\b", r"\bwarning\b", r"\bcaution\b", r"\bbeware\b", r"\bcareful\b",
            r"\bwatch out\b", r"\bavoid doing\b", r"\bnot recommended\b", r"\boffensive\b", r"\binappropriate\b", r"\btaboo\b", r"\bdisrespectful\b",
            r"\binsensitive\b", r"\brude\b", r"\bimpolite\b", r"\bunacceptable\b", r"\bunsuitable\b", r"\binsulting\b", r"\boffend\b", r"\boffends\b",
            r"\brefrain from\b", r"\bstay away from\b", r"\bkeep away from\b", r"\bmake sure not to\b", r"\bbe careful not to\b", r"\bensure you don't\b",
            r"\bit's best not to\b", r"\byou should not\b", r"\btry not to\b", r"\bhighly discouraged\b", r"\bstrongly advised against\b", r"\bnot wise to\b",
            r"\bbad idea to\b", r"\bunwise to\b"
        ]
        self.taboo_pattern = re.compile('|'.join(self.taboo_keywords), re.IGNORECASE)
        self.results = []
        self.culture_stats = defaultdict(int)
        self.category_stats = defaultdict(int)

    def get_culture_names(self):
        print("ğŸ” è·å–æ–‡åŒ–åˆ—è¡¨...")
        try:
            response = self.session.get(f"{self.base_url}/countries", timeout=15)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            culture_names = []
            for link in soup.find_all('a', href=True):
                href = link['href']
                if href.endswith('-culture') and href.startswith('/'):
                    culture_name = href[1:]
                    culture_names.append(culture_name)
            culture_names = list(set(culture_names))
            culture_names.sort()
            print(f"âœ… æ‰¾åˆ° {len(culture_names)} ä¸ªæ–‡åŒ–")
            return culture_names
        except Exception as e:
            print(f"âŒ è·å–æ–‡åŒ–åˆ—è¡¨å¤±è´¥: {e}")
            return []

    def extract_taboo_paragraphs(self, text):
        """æå–åŒ…å«ç¦å¿Œå…³é”®è¯çš„æ•´æ®µ"""
        paragraphs = re.split(r'\n{2,}|\r{2,}', text)
        paragraphs = [p.strip() for p in paragraphs if p.strip() and len(p.strip()) > 20]
        taboo_paragraphs = []
        for para in paragraphs:
            matches = list(self.taboo_pattern.finditer(para))
            if matches:
                clean_para = ' '.join(para.split())
                taboo_paragraphs.append({
                    'paragraph': clean_para,
                    'keywords': [m.group() for m in matches],
                    'length': len(clean_para)
                })
        return taboo_paragraphs

    def crawl_subcategory_page(self, culture_name, subcategory):
        culture_base = culture_name.replace('-culture', '')
        url = f"{self.base_url}/{culture_name}/{culture_base}-{subcategory}"
        try:
            response = self.session.get(url, timeout=10)
            if response.status_code == 404:
                alt_url = f"{self.base_url}/{culture_name}/{culture_name}-{subcategory}"
                response = self.session.get(alt_url, timeout=10)
                if response.status_code == 200:
                    url = alt_url
            if response.status_code != 200:
                return []
            soup = BeautifulSoup(response.content, 'html.parser')
            title_element = soup.find('title')
            page_title = title_element.get_text().strip() if title_element else f"{culture_name} - {subcategory}"
            main_content = (soup.find('main') or 
                           soup.find('article') or 
                           soup.find('div', class_=re.compile(r'content|main|body', re.I)))
            content_area = main_content if main_content else soup
            for unwanted in content_area(['script', 'style', 'nav', 'footer', 'header', 'aside']):
                unwanted.decompose()
            text_content = content_area.get_text()
            taboo_paragraphs = self.extract_taboo_paragraphs(text_content)
            if taboo_paragraphs:
                result = {
                    'culture': culture_name.replace('-culture', '').replace('-', ' ').title(),
                    'category': subcategory,
                    'url': url,
                    'title': page_title,
                    'taboo_paragraphs': taboo_paragraphs,
                    'count': len(taboo_paragraphs)
                }
                self.results.append(result)
                self.culture_stats[culture_name] += len(taboo_paragraphs)
                self.category_stats[subcategory] += len(taboo_paragraphs)
                print(f"    âœ… {subcategory}: æ‰¾åˆ° {len(taboo_paragraphs)} æ¡ç¦å¿Œæ®µè½")
                for i, para in enumerate(taboo_paragraphs[:2]):
                    print(f"       {i+1}. {para['paragraph'][:80]}...")
                return taboo_paragraphs
            return []
        except Exception as e:
            return []

    def crawl_culture_deep(self, culture_name):
        culture_display = culture_name.replace('-culture', '').replace('-', ' ').title()
        print(f"\nğŸŒ çˆ¬å–æ–‡åŒ–: {culture_display}")
        total_found = 0
        for subcategory in self.subcategories:
            taboo_content = self.crawl_subcategory_page(culture_name, subcategory)
            total_found += len(taboo_content)
            time.sleep(0.3)
        if total_found > 0:
            print(f"  ğŸ“Š æ€»è®¡æ‰¾åˆ° {total_found} æ¡ç¦å¿Œå†…å®¹")
        else:
            print(f"  â– æœªæ‰¾åˆ°ç¦å¿Œå†…å®¹")
        return total_found

    def crawl_all_cultures(self, max_cultures=None):
        culture_names = self.get_culture_names()
        if not culture_names:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•æ–‡åŒ–é¡µé¢")
            return []
        if max_cultures:
            culture_names = culture_names[:max_cultures]
            print(f"ğŸ”¢ é™åˆ¶çˆ¬å–å‰ {max_cultures} ä¸ªæ–‡åŒ–")
        print(f"\nğŸš€ å¼€å§‹æ·±åº¦çˆ¬å– {len(culture_names)} ä¸ªæ–‡åŒ–çš„è¯¦ç»†å†…å®¹...")
        print("=" * 80)
        total_cultures_with_content = 0
        total_sentences = 0
        for i, culture_name in enumerate(culture_names, 1):
            print(f"\n[{i}/{len(culture_names)}] ", end="")
            found_count = self.crawl_culture_deep(culture_name)
            if found_count > 0:
                total_cultures_with_content += 1
                total_sentences += found_count
        print(f"\n{'='*80}")
        print(f"ğŸ‰ çˆ¬å–å®Œæˆ!")
        print(f"ğŸ“ˆ æœ‰å†…å®¹çš„æ–‡åŒ–æ•°: {total_cultures_with_content}/{len(culture_names)}")
        print(f"ğŸ“ æ€»ç¦å¿Œå†…å®¹æ•°: {total_sentences}")
        return self.results

    def save_detailed_results(self, json_file="detailed_cultural_taboos_paragraph.json", csv_file="detailed_cultural_taboos_paragraph.csv"):
        if not self.results:
            print("âŒ æ²¡æœ‰ç»“æœéœ€è¦ä¿å­˜")
            return
        output_data = {
            'summary': {
                'total_cultures': len(set(r['culture'] for r in self.results)),
                'total_pages': len(self.results),
                'total_paragraphs': sum(r['count'] for r in self.results),
                'crawl_date': time.strftime('%Y-%m-%d %H:%M:%S')
            },
            'culture_stats': dict(self.culture_stats),
            'category_stats': dict(self.category_stats),
            'detailed_results': self.results
        }
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        print(f"âœ… è¯¦ç»†JSONç»“æœå·²ä¿å­˜: {json_file}")
        with open(csv_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow(['æ–‡åŒ–', 'ç±»åˆ«', 'URL', 'é¡µé¢æ ‡é¢˜', 'ç¦å¿Œæ®µè½', 'å…³é”®è¯', 'æ®µè½é•¿åº¦'])
            for page in self.results:
                for para in page['taboo_paragraphs']:
                    writer.writerow([
                        page['culture'],
                        page['category'],
                        page['url'],
                        page['title'],
                        para['paragraph'],
                        ', '.join(para['keywords']),
                        para['length']
                    ])
        print(f"âœ… è¯¦ç»†CSVç»“æœå·²ä¿å­˜: {csv_file}")

    def print_comprehensive_summary(self):
        if not self.results:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•ç¦å¿Œå†…å®¹")
            return
        total_paragraphs = sum(r['count'] for r in self.results)
        total_cultures = len(set(r['culture'] for r in self.results))
        print(f"\n{'='*80}")
        print(f"ğŸ“Š Cultural Atlas æ·±åº¦ç¦å¿Œå†…å®¹åˆ†ææŠ¥å‘Šï¼ˆæ•´æ®µç‰ˆï¼‰")
        print(f"{'='*80}")
        print(f"ğŸŒ æ¶‰åŠæ–‡åŒ–æ•°é‡: {total_cultures}")
        print(f"ğŸ“„ æœ‰å†…å®¹çš„é¡µé¢æ•°: {len(self.results)}")
        print(f"ğŸ“ æ€»ç¦å¿Œæ®µè½æ•°: {total_paragraphs}")
        print(f"ğŸ“ˆ å¹³å‡æ¯æ–‡åŒ–: {total_paragraphs/total_cultures:.1f} æ¡")
        print(f"\nğŸ“‹ æŒ‰å†…å®¹ç±»åˆ«ç»Ÿè®¡:")
        sorted_categories = sorted(self.category_stats.items(), key=lambda x: x[1], reverse=True)
        for category, count in sorted_categories:
            print(f"  ğŸ“Œ {category:<25} {count:3d} æ¡")
        print(f"\nğŸŒ ç¦å¿Œå†…å®¹æœ€å¤šçš„æ–‡åŒ– (Top 10):")
        sorted_cultures = sorted(self.culture_stats.items(), key=lambda x: x[1], reverse=True)[:10]
        for i, (culture, count) in enumerate(sorted_cultures, 1):
            display_name = culture.replace('-culture', '').replace('-', ' ').title()
            print(f"  {i:2d}. {display_name:<25} {count:3d} æ¡")
        keyword_count = defaultdict(int)
        for result in self.results:
            for para in result['taboo_paragraphs']:
                for keyword in para['keywords']:
                    keyword_count[keyword.lower()] += 1
        print(f"\nğŸ”¤ æœ€å¸¸è§çš„ç¦å¿Œå…³é”®è¯ (Top 15):")
        sorted_keywords = sorted(keyword_count.items(), key=lambda x: x[1], reverse=True)[:15]
        for i, (keyword, count) in enumerate(sorted_keywords, 1):
            print(f"  {i:2d}. {keyword:<20} {count:3d} æ¬¡")
        print(f"\nğŸ“„ ç¦å¿Œå†…å®¹ç²¾é€‰ç¤ºä¾‹:")
        example_count = 0
        for result in self.results[:5]:
            if example_count >= 10:
                break
            print(f"\nğŸŒ ã€{result['culture']}ã€‘- {result['category']}")
            for para in result['taboo_paragraphs'][:2]:
                if example_count >= 10:
                    break
                print(f"   ğŸ’¬ {para['paragraph'][:150]}...")
                print(f"      ğŸ·ï¸  å…³é”®è¯: {', '.join(para['keywords'])}")
                example_count += 1

def main():
    print("ğŸŒ Cultural Atlas æ·±åº¦ç¦å¿Œå†…å®¹çˆ¬è™«ï¼ˆæ•´æ®µæå–ç‰ˆï¼‰")
    print("ğŸ“Š åŸºäºçœŸå®ç½‘ç«™ç»“æ„çš„å®Œæ•´ç‰ˆæœ¬")
    print("ğŸ¯ çˆ¬å–è·¯å¾„: å›½å®¶åˆ—è¡¨ â†’ æ–‡åŒ–ä¸»é¡µ â†’ å…·ä½“å†…å®¹å­é¡µé¢")
    print("="*80)
    crawler = CulturalAtlasDeepCrawler()
    try:
        results = crawler.crawl_all_cultures()
        crawler.print_comprehensive_summary()
        if results:
            crawler.save_detailed_results()
            print(f"\nâœ¨ ä»»åŠ¡å®Œæˆï¼ç”Ÿæˆçš„æ–‡ä»¶:")
            print(f"   ğŸ“ detailed_cultural_taboos_paragraph.json - åŒ…å«ç»Ÿè®¡ä¿¡æ¯çš„å®Œæ•´JSONæ•°æ®")
            print(f"   ğŸ“Š detailed_cultural_taboos_paragraph.csv - è¯¦ç»†CSVè¡¨æ ¼æ•°æ®")
    except KeyboardInterrupt:
        print(f"\nâš ï¸  ç”¨æˆ·ä¸­æ–­äº†çˆ¬å–è¿‡ç¨‹")
        if crawler.results:
            print("ğŸ’¾ ä¿å­˜å·²è·å–çš„éƒ¨åˆ†ç»“æœ...")
            crawler.save_detailed_results()
    except Exception as e:
        print(f"\nâŒ çˆ¬å–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        if crawler.results:
            print("ğŸ’¾ ä¿å­˜å·²è·å–çš„éƒ¨åˆ†ç»“æœ...")
            crawler.save_detailed_results()

if __name__ == "__main__":
    main()
